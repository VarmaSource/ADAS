{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b726ccae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c25499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Dataset Preparation\n",
    "# Organize your dataset with labeled images (speed hump, plain road, pothole)\n",
    "# Directory structure:\n",
    "# - train\n",
    "#   - Humps\n",
    "#   - Plain\n",
    "#   - pothole\n",
    "# - test\n",
    "#   - Humps\n",
    "#   - Plain\n",
    "#   - pothole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81cf8d44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41436d90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3: Model Architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(224, 224, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))  # 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1fe0808",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1361 images belonging to 3 classes.\n",
      "Found 102 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Split Dataset\n",
    "train_dir = \"C:/Users/KIIT/OneDrive/Documents/DS-1/My Dataset/train\"\n",
    "test_dir = \"C:/Users/KIIT/OneDrive/Documents/DS-1/My Dataset/test\"\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11e86dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      " 5/43 [==>...........................] - ETA: 2:15 - loss: 6.8735 - accuracy: 0.3375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIIT\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 142s 3s/step - loss: 1.5850 - accuracy: 0.5849 - val_loss: 2.6360 - val_accuracy: 0.3824\n",
      "Epoch 2/25\n",
      "43/43 [==============================] - 157s 4s/step - loss: 0.3465 - accuracy: 0.8677 - val_loss: 3.2545 - val_accuracy: 0.4902\n",
      "Epoch 3/25\n",
      "43/43 [==============================] - 138s 3s/step - loss: 0.2832 - accuracy: 0.9008 - val_loss: 3.7203 - val_accuracy: 0.4804\n",
      "Epoch 4/25\n",
      "43/43 [==============================] - 143s 3s/step - loss: 0.2883 - accuracy: 0.8876 - val_loss: 3.8639 - val_accuracy: 0.4902\n",
      "Epoch 5/25\n",
      "43/43 [==============================] - 141s 3s/step - loss: 0.2385 - accuracy: 0.9155 - val_loss: 4.1256 - val_accuracy: 0.5490\n",
      "Epoch 6/25\n",
      "43/43 [==============================] - 138s 3s/step - loss: 0.1925 - accuracy: 0.9302 - val_loss: 4.7853 - val_accuracy: 0.4608\n",
      "Epoch 7/25\n",
      "43/43 [==============================] - 151s 3s/step - loss: 0.2122 - accuracy: 0.9206 - val_loss: 4.1124 - val_accuracy: 0.4706\n",
      "Epoch 8/25\n",
      "43/43 [==============================] - 163s 4s/step - loss: 0.1917 - accuracy: 0.9287 - val_loss: 4.4461 - val_accuracy: 0.5000\n",
      "Epoch 9/25\n",
      "43/43 [==============================] - 136s 3s/step - loss: 0.1285 - accuracy: 0.9566 - val_loss: 4.8616 - val_accuracy: 0.4608\n",
      "Epoch 10/25\n",
      "43/43 [==============================] - 137s 3s/step - loss: 0.1341 - accuracy: 0.9559 - val_loss: 4.7552 - val_accuracy: 0.5000\n",
      "Epoch 11/25\n",
      "43/43 [==============================] - 137s 3s/step - loss: 0.1503 - accuracy: 0.9449 - val_loss: 4.6434 - val_accuracy: 0.4216\n",
      "Epoch 12/25\n",
      "43/43 [==============================] - 135s 3s/step - loss: 0.1491 - accuracy: 0.9449 - val_loss: 4.9338 - val_accuracy: 0.4902\n",
      "Epoch 13/25\n",
      "43/43 [==============================] - 141s 3s/step - loss: 0.1224 - accuracy: 0.9544 - val_loss: 5.1270 - val_accuracy: 0.4804\n",
      "Epoch 14/25\n",
      "43/43 [==============================] - 256s 6s/step - loss: 0.1127 - accuracy: 0.9611 - val_loss: 5.1285 - val_accuracy: 0.4020\n",
      "Epoch 15/25\n",
      "43/43 [==============================] - 171s 4s/step - loss: 0.1214 - accuracy: 0.9559 - val_loss: 5.5572 - val_accuracy: 0.4706\n",
      "Epoch 16/25\n",
      "43/43 [==============================] - 137s 3s/step - loss: 0.1113 - accuracy: 0.9552 - val_loss: 5.0634 - val_accuracy: 0.4804\n",
      "Epoch 17/25\n",
      "43/43 [==============================] - 188s 4s/step - loss: 0.1075 - accuracy: 0.9603 - val_loss: 5.0359 - val_accuracy: 0.5196\n",
      "Epoch 18/25\n",
      "43/43 [==============================] - 141s 3s/step - loss: 0.0800 - accuracy: 0.9706 - val_loss: 5.7102 - val_accuracy: 0.4902\n",
      "Epoch 19/25\n",
      "43/43 [==============================] - 159s 4s/step - loss: 0.0605 - accuracy: 0.9765 - val_loss: 4.7591 - val_accuracy: 0.5098\n",
      "Epoch 20/25\n",
      "43/43 [==============================] - 162s 4s/step - loss: 0.0944 - accuracy: 0.9603 - val_loss: 5.4606 - val_accuracy: 0.4706\n",
      "Epoch 21/25\n",
      "43/43 [==============================] - 154s 4s/step - loss: 0.0725 - accuracy: 0.9706 - val_loss: 7.3011 - val_accuracy: 0.4706\n",
      "Epoch 22/25\n",
      "43/43 [==============================] - 147s 3s/step - loss: 0.0715 - accuracy: 0.9706 - val_loss: 5.8838 - val_accuracy: 0.5000\n",
      "Epoch 23/25\n",
      "43/43 [==============================] - 143s 3s/step - loss: 0.0656 - accuracy: 0.9780 - val_loss: 5.6533 - val_accuracy: 0.5098\n",
      "Epoch 24/25\n",
      "43/43 [==============================] - 152s 4s/step - loss: 0.0481 - accuracy: 0.9838 - val_loss: 6.1647 - val_accuracy: 0.5196\n",
      "Epoch 25/25\n",
      "43/43 [==============================] - 161s 4s/step - loss: 0.0408 - accuracy: 0.9853 - val_loss: 7.4690 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2442041c730>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Model Training\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=25,  # Adjust the number of epochs as needed\n",
    "    validation_data=test_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca3bb9c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 160ms/step\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1272: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_class_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, (\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m50\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Display the result\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mObject Recognition\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Break the loop if 'ESC' key is pressed\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m30\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m27\u001b[39m:\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1272: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Real-time Video Processing\n",
    "cap = cv2.VideoCapture(\"D:\\speed_humps.mp4\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame for the model\n",
    "    img = cv2.resize(frame, (224, 224))\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img / 255.0  # Normalize pixel values\n",
    "\n",
    "    # Make predictions using the trained model\n",
    "    predictions = model.predict(img)\n",
    "\n",
    "    # Get the predicted class\n",
    "    predicted_class_index = np.argmax(predictions)\n",
    "    classes = ['Speed Hump', 'Plain Road', 'Pothole']\n",
    "    predicted_class_label = classes[predicted_class_index]\n",
    "\n",
    "    # Visualize the result on the frame\n",
    "    cv2.putText(frame, f'Prediction: {predicted_class_label}', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Display the result\n",
    "    cv2.imshow('Object Recognition', frame)\n",
    "\n",
    "    # Break the loop if 'ESC' key is pressed\n",
    "    if cv2.waitKey(30) == 27:\n",
    "        break\n",
    "\n",
    "# Release the capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36cd1e0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('C:/Users/KIIT/OneDrive/Documents/Research/Patent/Vehicle/CNN_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7f8450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db680662-dff0-4a1a-87d2-844a3c500d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619bcd85-71fd-4d19-bf5a-1d4e7e8bc181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
